{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adult Income Prediction Analysis\n",
    "This notebook trains a Random Forest classifier to predict income levels from the Adult dataset and includes embedded visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    # Import necessary libraries
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
    from sklearn.compose import ColumnTransformer
    from sklearn.pipeline import Pipeline
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
    import joblib
    import os

    # Set up matplotlib for Jupyter inline display
    %matplotlib inline
    plt.rcParams["font.family"] = ["SimHei", "WenQuanYi Micro Hei", "Heiti TC"]  # 支持中文显示 (Support Chinese display)
    sns.set(font_scale=1.2)
    sns.set_style("whitegrid")
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Path Configuration and Data Loading\n",
    "Configure file paths and load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    # Configure paths
    data_path = os.path.join("Adult", "adult", "adult.data")
    results_root = r"D:\german\Kernel-ridge-regression\Adult\results"
    model_dir = os.path.join(results_root, "models")
    figures_dir = os.path.join(results_root, "figures")
    os.makedirs(model_dir, exist_ok=True)
    os.makedirs(figures_dir, exist_ok=True)
    print(f"Results will be saved to: {results_root}")

    # Define column names
    column_names = [
        "age", "workclass", "fnlwgt", "education", "education-num",
        "marital-status", "occupation", "relationship", "race", "sex",
        "capital-gain", "capital-loss", "hours-per-week", "native-country", "income"
    ]

    # Load dataset
    try:
        data = pd.read_csv(data_path, header=None, names=column_names)
        print(f"Dataset loaded successfully, {data.shape[0]} rows, {data.shape[1]} columns")
    except FileNotFoundError:
        print(f"Error: Dataset file not found, please check the path: {data_path}")
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    # Basic data information
    print("\n===== Basic Data Information =====")
    print(data.info())

    # Statistical summary
    print("\n===== Data Statistical Summary =====")
    print(data.describe())

    # Check for missing values
    print("\n===== Missing Value Statistics =====")
    for col in data.columns:
        missing_count = (data[col] == '?').sum()
        if missing_count > 0:
            print(f"{col}: {missing_count} missing values ({missing_count/len(data):.2%})")

    # Handle missing values with mode
    for col in data.columns:
        if (data[col] == '?').sum() > 0:
            mode_val = data[data[col] != '?'][col].mode()[0]
            data[col] = data[col].replace('?', mode_val)
            print(f"Filled missing values in {col} with mode '{mode_val}'")
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    # Income distribution visualization
    plt.figure(figsize=(8, 6))
    sns.countplot(x="income", data=data)
    plt.title("Income Distribution")
    plt.tight_layout()
    plt.show()  # Displays in notebook

    # Save to file as well
    income_dist_path = os.path.join(figures_dir, "income_distribution.png")
    plt.savefig(income_dist_path)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    # Age vs Income visualization
    plt.figure(figsize=(10, 6))
    sns.boxplot(x="income", y="age", data=data)
    plt.title("Relationship between Age and Income")
    plt.tight_layout()
    plt.show()  # Displays in notebook

    # Save to file
    age_income_path = os.path.join(figures_dir, "age_vs_income.png")
    plt.savefig(age_income_path)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    # Prepare features and target
    X = data.drop("income", axis=1)
    y = data["income"]

    # Encode target variable
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    # Identify feature types
    categorical_features = ["workclass", "education", "marital-status", 
                           "occupation", "relationship", "race", "sex", "native-country"]
    numerical_features = ["age", "fnlwgt", "education-num", "capital-gain", 
                         "capital-loss", "hours-per-week"]

    # Create preprocessing pipeline
    preprocessor = ColumnTransformer(
        transformers=[
            ("num", StandardScaler(), numerical_features),
            ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), categorical_features)
        ]
    )

    # Split into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    _, _, y_train_encoded, y_test_encoded = train_test_split(
        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
    )

    print(f"Training set samples: {X_train.shape[0]}, Test set samples: {X_test.shape[0]}")

    # Train Random Forest model
    print("\n===== Starting Random Forest Model Training =====")
    model = Pipeline(steps=[
        ("preprocessor", preprocessor),
        ("classifier", RandomForestClassifier(
            n_estimators=100, 
            max_depth=10,
            min_samples_split=10,
            random_state=42,
            n_jobs=-1
        ))
    ])

    # Cross-validation
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    print(f"Cross-validation accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}")

    # Fit model
    model.fit(X_train, y_train)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    # Make predictions
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]

    # Basic evaluation
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Test set accuracy: {accuracy:.4f}")

    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    # Confusion Matrix visualization
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", 
                xticklabels=model.classes_, 
                yticklabels=model.classes_)
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title("Confusion Matrix")
    plt.tight_layout()
    plt.show()  # Displays in notebook

    # Save to file
    cm_path = os.path.join(figures_dir, "confusion_matrix.png")
    plt.savefig(cm_path)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    # ROC Curve visualization
    fpr, tpr, _ = roc_curve(y_test_encoded, y_pred_proba)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (Area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc="lower right")
    plt.tight_layout()
    plt.show()  # Displays in notebook

    # Save to file
    roc_path = os.path.join(figures_dir, "roc_curve.png")
    plt.savefig(roc_path)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    # Get feature names after preprocessing
    ohe = model.named_steps['preprocessor'].named_transformers_['cat']
    cat_feature_names = list(ohe.get_feature_names_out(categorical_features))
    all_feature_names = numerical_features + cat_feature_names

    # Get feature importances
    importances = model.named_steps['classifier'].feature_importances_
    indices = np.argsort(importances)[::-1]

    # Print top 10 features
    print("\n===== Important Features (Top 10) =====")
    for f in range(min(10, len(all_feature_names))):
        print(f"{all_feature_names[indices[f]]}: {importances[indices[f]]:.4f}")

    # Visualize feature importance
    plt.figure(figsize=(12, 8))
    plt.barh(range(min(15, len(all_feature_names))), 
             importances[indices[:15]], 
             align='center')
    plt.yticks(range(min(15, len(all_feature_names))), 
               [all_feature_names[i] for i in indices[:15]])
    plt.xlabel('Feature Importance')
    plt.title('Random Forest Feature Importance (Top 15)')
    plt.gca().invert_yaxis()  # Most important at the top
    plt.tight_layout()
    plt.show()  # Displays in notebook

    # Save to file
    fi_path = os.path.join(figures_dir, "feature_importance.png")
    plt.savefig(fi_path)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Model and Example Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    # Save the trained model
    model_path = os.path.join(model_dir, "adult_income_model.pkl")
    joblib.dump(model, model_path)
    print(f"Model saved to: {model_path}")

    # Example predictions
    print("\n===== Example Predictions =====")
    sample_indices = np.random.choice(X_test.index, 5, replace=False)
    sample_data = X_test.loc[sample_indices]
    sample_actual = y_test.loc[sample_indices]
    sample_pred = model.predict(sample_data)

    # Display results
    result_df = pd.DataFrame({
        'Actual Income': sample_actual,
        'Predicted Income': sample_pred
    })
    print(result_df)
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
    